{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def split_web_content(url):\n",
    "    # Make a request to the URL and get the content\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    for paragraph in paragraphs:\n",
    "        text = paragraph.get_text()\n",
    "        current_chunk.append(text)\n",
    "        # Add the current chunk to the list of chunks when a new paragraph is detected\n",
    "        if text.endswith('.'):\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "    # Add the final chunk to the list of chunks\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 11\n",
      "Chunk 1:\n",
      "Front page layout Site theme \n",
      "Benj Edwards\n",
      "    -  Apr 6, 2023 3:58 pm UTC\n",
      " Over the past few months, AI chatbots like ChatGPT have captured the world's attention due to their ability to converse in a human-like way on just about any subject. But they come with a serious drawback: They can present convincing false information easily, making them unreliable sources of factual information and potential sources of defamation.\n",
      "\n",
      "Chunk 2:\n",
      "Why do AI chatbots make things up, and will we ever be able to fully trust their output? We asked several experts and dug into how these AI models work to find the answers.\n",
      "\n",
      "Chunk 3:\n",
      "AI chatbots such as OpenAI's ChatGPT rely on a type of AI called a \"large language model\" (LLM) to generate their responses. An LLM is a computer program trained on millions of text sources that can read and generate \"natural language\" text—language as humans would naturally write or talk. Unfortunately, they can also make mistakes.\n",
      "\n",
      "Chunk 4:\n",
      "In academic literature, AI researchers often call these mistakes \"hallucinations.\" But that label has grown controversial as the topic becomes mainstream because some people feel it anthropomorphizes AI models (suggesting they have human-like features) or gives them agency (suggesting they can make their own choices) in situations where that should not be implied. The creators of commercial LLMs may also use hallucinations as an excuse to blame the AI model for faulty outputs instead of taking responsibility for the outputs themselves.\n",
      "\n",
      "Chunk 5:\n",
      "Still, generative AI is so new that we need metaphors borrowed from existing ideas to explain these highly technical concepts to the broader public. In this vein, we feel the term \"confabulation,\" although similarly imperfect, is a better metaphor than \"hallucination.\" In human psychology, a \"confabulation\" occurs when someone's memory has a gap and the brain convincingly fills in the rest without intending to deceive others. ChatGPT does not work like the human brain, but the term \"confabulation\" arguably serves as a better metaphor because there's a creative gap-filling principle at work, as we'll explore below.\n",
      "\n",
      "Chunk 6:\n",
      "It's a big problem when an AI bot generates false information that can potentially mislead, misinform, or defame. Recently, The Washington Post reported on a law professor who discovered that ChatGPT had placed him on a list of legal scholars who had sexually harassed someone. But it never happened—ChatGPT made it up. The same day, Ars reported on an Australian mayor who allegedly found that ChatGPT claimed he had been convicted of bribery and sentenced to prison, a complete fabrication.\n",
      "\n",
      "Chunk 7:\n",
      "Shortly after ChatGPT's launch, people began proclaiming the end of the search engine. At the same time, though, many examples of ChatGPT's confabulations began to circulate on social media. The AI bot has invented books and studies that don't exist, publications that professors didn't write, fake academic papers, false legal citations, non-existent Linux system features, unreal retail mascots, and technical details that don't make sense.\n",
      "\n",
      "Chunk 8:\n",
      "Curious how GPT will replace Google if it gives wrong answers with high confidence.For example, I asked ChatGPT to give a list of top books on Social Cognitive Theory. Out of the 10 books on the answer, 4 books don't exist and 3 books were written by different people. pic.twitter.com/b2jN9VNCFv And yet despite ChatGPT's predilection for casually fibbing, counter-intuitively, its resistance to confabulation is why we're even talking about it today. Some experts note that ChatGPT was technically an improvement over vanilla GPT-3 (its predecessor model) because it could refuse to answer some questions or let you know when its answers might not be accurate.\n",
      "\n",
      "Chunk 9:\n",
      "\"A major factor in Chat's success is that it manages to suppress confabulation enough to make it unnoticeable for many common questions,\" said Riley Goodside, an expert in large language models who serves as staff prompt engineer at Scale AI. \"Compared to its predecessors, ChatGPT is notably less prone to making things up.\" If used as a brainstorming tool, ChatGPT's logical leaps and confabulations might lead to creative breakthroughs. But when used as a factual reference, ChatGPT could cause real harm, and OpenAI knows it.\n",
      "\n",
      "Chunk 10:\n",
      "Not long after the model's launch, OpenAI CEO Sam Altman tweeted, \"ChatGPT is incredibly limited, but good enough at some things to create a misleading impression of greatness. It's a mistake to be relying on it for anything important right now. It’s a preview of progress; we have lots of work to do on robustness and truthfulness.\" In a later tweet, he wrote, \"It does know a lot, but the danger is that it is confident and wrong a significant fraction of the time.\" What's going on here? Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox.\n",
      "\n",
      "Chunk 11:\n",
      "\n",
      "  CNMN Collection\n",
      "  WIRED Media Group\n",
      "  © 2023 Condé Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Ars Technica Addendum (effective 8/21/2018). Ars may earn compensation on sales from links on this site. Read our affiliate link policy.\n",
      "Your California Privacy Rights | Do Not Sell My Personal Information\n",
      "  The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.\n",
      "Ad Choices\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\"\n",
    "chunks = split_web_content(url)\n",
    "\n",
    "\n",
    "# Print out the number of chunks and all the chunks\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
